{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd358a5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c482c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1fa61",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d3fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/data_science_job_posts_2025.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82baa7",
   "metadata": {},
   "source": [
    "### Schema overview & missing values profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd19054",
   "metadata": {},
   "source": [
    "## Dataset Cleaning \n",
    "\n",
    "This module provides a structured data-cleaning pipeline for the\n",
    "*Data Science Job Postings 2025* dataset.\n",
    "\n",
    "**Functionality:**\n",
    "\n",
    "- **State Extraction:** Uses regular expressions to extract U.S. state abbreviations from the `headquarter` column and maps them to FIPS codes (`state`, `fips`, `fips_int` columns).\n",
    "- **Salary Parsing:** Converts salary text ranges (e.g., \"$80K–$120K\") into numeric midpoints via normalization and numeric extraction (`salary_mid`).\n",
    "- **Seniority Normalization:** Cleans and standardizes the `seniority_level` column to lowercase values, replacing invalid entries.\n",
    "- **Skills Parsing:** Transforms stringified skill lists into clean Python lists (`skills_list`) by stripping brackets, quotes, and whitespace.\n",
    "- **Deduplication:** Removes duplicate job postings using key columns (`job_title`, `company`, `location`, `post_date`, `salary`, `skills`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47b0fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed cleaning. new cleaned Dataset created at → ../data/data_science_job_posts_2025_clean.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- Constants ---\n",
    "STATE_ABBR = r\"\\b(AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VT|VA|WA|WV|WI|WY|DC)\\b\"\n",
    "_ABBR_TO_FIPS = {\n",
    "    'AL':'01','AK':'02','AZ':'04','AR':'05','CA':'06','CO':'08','CT':'09','DE':'10','DC':'11','FL':'12','GA':'13',\n",
    "    'HI':'15','ID':'16','IL':'17','IN':'18','IA':'19','KS':'20','KY':'21','LA':'22','ME':'23','MD':'24','MA':'25',\n",
    "    'MI':'26','MN':'27','MS':'28','MO':'29','MT':'30','NE':'31','NV':'32','NH':'33','NJ':'34','NM':'35','NY':'36',\n",
    "    'NC':'37','ND':'38','OH':'39','OK':'40','OR':'41','PA':'42','RI':'44','SC':'45','SD':'46','TN':'47','TX':'48',\n",
    "    'UT':'49','VT':'50','VA':'51','WA':'53','WV':'54','WI':'55','WY':'56'\n",
    "}\n",
    "\n",
    "_state_pat = re.compile(STATE_ABBR)\n",
    "\n",
    "def clean_skills(s):\n",
    "    s = str(s)\n",
    "    s = s.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "    s = s.replace(\",\", \" \")\n",
    "    return s\n",
    "\n",
    "df['skills_clean'] = df['skills'].apply(clean_skills)\n",
    "\n",
    "# Parse salary: handle \"€70,000-€90,000\" or \"€85,000\"\n",
    "def parse_salary_to_number(s):\n",
    "    s = str(s).replace(\"€\", \"\").replace(\" \", \"\")\n",
    "    # Range: \"70000-90000\"\n",
    "    if \"-\" in s:\n",
    "        parts = s.split(\"-\")\n",
    "        try:\n",
    "            low = float(parts[0].replace(\",\", \"\"))\n",
    "            high = float(parts[1].replace(\",\", \"\"))\n",
    "            return (low + high) / 2.0\n",
    "        except:\n",
    "            return np.nan\n",
    "    else:\n",
    "        try:\n",
    "            return float(s.replace(\",\", \"\"))\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "def extract_state(headquarter: pd.Series) -> pd.Series:\n",
    "    def _one(x):\n",
    "        s = str(x).upper()\n",
    "        m = _state_pat.search(s)\n",
    "        return m.group(1) if m else None\n",
    "    return headquarter.astype(str).map(_one)\n",
    "\n",
    "\n",
    "def parse_salary(s):\n",
    "    \"\"\"Return a numeric midpoint if salary is a range or single number, else None.\"\"\"\n",
    "    s = str(s)\n",
    "    if not s or s.lower() in {\"nan\", \"none\"}:\n",
    "        return None\n",
    "    s_norm = s.replace(\"–\", \"-\").lower()\n",
    "    nums = [float(x.replace(\",\", \"\")) for x in re.findall(r\"\\d[\\d,]*\\.?\\d*\", s_norm)]\n",
    "    if not nums:\n",
    "        return None\n",
    "    mid = (nums[0] + nums[1]) / 2 if len(nums) >= 2 else nums[0]\n",
    "    if \"k\" in s_norm:\n",
    "        mid *= 1000.0\n",
    "    return mid\n",
    "\n",
    "def company_size_group(size):\n",
    "    size_str = str(size)\n",
    "    \n",
    "    # Very big ones like \"€352.44B\" → treat as large\n",
    "    if \"€\" in size_str or \"b\" in size_str.lower() or \"m\" in size_str.lower():\n",
    "        return \"large\"\n",
    "    \n",
    "    cleaned = size_str.replace(\",\", \"\")\n",
    "    try:\n",
    "        n = int(cleaned)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    if n < 1000:\n",
    "        return \"small\"\n",
    "    elif n < 10000:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "def parse_skills(s):\n",
    "    \"\"\"\n",
    "    Turn \"['python', 'sql']\" or '[\"python\",\"sql\"]' or \"python, sql\" into a list.\n",
    "    Keep it simple and lowercase.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    parts = [x.strip(\" '\\\"\").lower() for x in s.strip(\"[]\").split(\",\") if x.strip(\" '\\\"\")]\n",
    "    return parts\n",
    "\n",
    "def parse_company_size(size):\n",
    "    if pd.isna(size):\n",
    "        return np.nan\n",
    "\n",
    "    s = str(size).strip()\n",
    "\n",
    "    # Case: \"Private\", \"Public\", \"Unknown\", \"Self-employed\"\n",
    "    if s.isalpha():\n",
    "        return np.nan\n",
    "\n",
    "    # Remove currency, scale symbols\n",
    "    s = s.replace(\"€\", \"\").replace(\"$\", \"\")\n",
    "    s = s.replace(\"B\", \"\").replace(\"M\", \"\").replace(\"K\", \"\")\n",
    "    s = s.replace(\",\", \"\").strip()\n",
    "\n",
    "    # If contains letters after cleaning → invalid\n",
    "    if any(c.isalpha() for c in s):\n",
    "        return np.nan\n",
    "\n",
    "    # Empty after cleaning\n",
    "    if s == \"\":\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def clean_jobs_df(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # --- state + fips ---\n",
    "    df[\"state\"] = extract_state(df.get(\"headquarter\"))\n",
    "    df[\"fips\"] = df[\"state\"].map(_ABBR_TO_FIPS)\n",
    "    df[\"fips_int\"] = pd.to_numeric(df[\"fips\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # --- salary ---\n",
    "    df[\"salary_mid\"] = df.get(\"salary\", pd.Series([None] * len(df))).apply(parse_salary)\n",
    "\n",
    "    # --- seniority ---\n",
    "    df[\"seniority_level_norm\"] = (\n",
    "        df.get(\"seniority_level\", pd.Series([None] * len(df)))\n",
    "          .astype(str).str.lower().str.strip()\n",
    "          .replace({\"nan\": pd.NA, \"\": pd.NA})\n",
    "    )\n",
    "\n",
    "    # --- skills ---\n",
    "    skills_parsed = df.get(\"skills\", pd.Series([None] * len(df))).apply(parse_skills)\n",
    "    df[\"skills_list\"] = skills_parsed\n",
    "    # CSV-friendly version for Altair notebook\n",
    "    df[\"skills_clean\"] = skills_parsed.apply(lambda lst: \"|\".join(lst))\n",
    "\n",
    "    # --- de-dup ---\n",
    "    dedup_keys = [\"job_title\", \"company\", \"location\", \"post_date\", \"salary\", \"skills\"]\n",
    "    existing = [c for c in dedup_keys if c in df.columns]\n",
    "    if existing:\n",
    "        df = df.drop_duplicates(subset=existing)\n",
    "\n",
    "    df[\"company_size_num\"] = df[\"company_size\"].apply(parse_company_size)\n",
    "    df['salary_usd'] = df['salary'].apply(parse_salary_to_number)\n",
    "    df = df.dropna(subset=['salary_usd'])\n",
    "    df['company_size_group'] = df['company_size'].apply(company_size_group)\n",
    "    df = df[df['company_size_group'] != 'unknown']\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    in_path = \"../data/data_science_job_posts_2025.csv\"\n",
    "    out_path = \"../data/data_science_job_posts_2025_clean.csv\"\n",
    "    raw = pd.read_csv(in_path, low_memory=False)\n",
    "    clean = clean_jobs_df(raw)\n",
    "    clean.to_csv(out_path, index=False)\n",
    "    print(f\"Completed cleaning. new cleaned Dataset created at → {out_path}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
